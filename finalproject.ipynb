{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4604 Final Project - Predicting if cancer is benign or not\n",
    "\n",
    "## Amogh Jahagirdar and Ryan Rouleau\n",
    "\n",
    "Dataset: [https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precursor Analysis/General Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:40.281643Z",
     "start_time": "2018-12-07T02:06:40.266921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:40.514272Z",
     "start_time": "2018-12-07T02:06:40.405812Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean     ...       texture_worst  perimeter_worst  \\\n",
       "count     569.000000     ...          569.000000       569.000000   \n",
       "mean        0.181162     ...           25.677223       107.261213   \n",
       "std         0.027414     ...            6.146258        33.602542   \n",
       "min         0.106000     ...           12.020000        50.410000   \n",
       "25%         0.161900     ...           21.080000        84.110000   \n",
       "50%         0.179200     ...           25.410000        97.660000   \n",
       "75%         0.195700     ...           29.720000       125.400000   \n",
       "max         0.304000     ...           49.540000       251.200000   \n",
       "\n",
       "        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "       Unnamed: 32  \n",
       "count          0.0  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:40.566759Z",
     "start_time": "2018-12-07T02:06:40.558749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop last column (\"Unnamed:32\" column of NaNs being read, when the CSV is opened up in Excel that column doesn't exist)\n",
    "data_cleaned = data.iloc[:, :-1]\n",
    "#Drop ID (just a bookeeping column part of the original data)\n",
    "data_cleaned = data_cleaned.drop(\"id\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:40.686457Z",
     "start_time": "2018-12-07T02:06:40.673738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages of benign and maligant data is \n",
      " B    62.741652\n",
      "M    37.258348\n",
      "Name: diagnosis, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Analyze the balance of the data.\n",
    "nrows = data_cleaned.shape[0]\n",
    "print(\"Percentages of benign and maligant data is \\n {}\".format(100 * data_cleaned[\"diagnosis\"].value_counts()/nrows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, there are significantly more benign cases than malignant in the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:40.936445Z",
     "start_time": "2018-12-07T02:06:40.928245Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_cleaned[data_cleaned.columns.difference([\"diagnosis\"])]\n",
    "y = data_cleaned['diagnosis']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Next, we will create a simple baseline classifier with no feature extraction. We can use scikit learn's DummyClassifier class with \"the most frequent\" strategy. This is not used for actual classification purposes, it is mereley a benchmark for what a theoretical classifier would predict if it didn't actually learn from the features in the data (a minimum accuracy for our actual models). All of our models should perform much better than the DummyClasifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:06:41.227863Z",
     "start_time": "2018-12-07T02:06:41.210043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training accuracy: 0.629108\n",
      "Baseline Testing accuracy: 0.622378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "baseline = DummyClassifier(strategy='most_frequent', random_state=1234)\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Baseline Training accuracy: %0.6f\" % accuracy_score(Y_train, baseline.predict(X_train)))\n",
    "print(\"Baseline Testing accuracy: %0.6f\" % accuracy_score(Y_test, baseline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models should be able to perform significantly above 60% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms (Baseline)\n",
    "###  Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:08:56.738850Z",
     "start_time": "2018-12-07T02:08:56.724216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Decision Tree Training accuracy: 1.000000\n",
      "Baseline Decision Tree Testing accuracy: 0.958042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(random_state=1234)\n",
    "decisionTree.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Baseline Decision Tree Training accuracy: %0.6f\" % accuracy_score(Y_train, decisionTree.predict(X_train)))\n",
    "print(\"Baseline Decision Tree Testing accuracy: %0.6f\" % accuracy_score(Y_test, decisionTree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, a decision tree classifier with no modification of hyperparamters severly overfits with a training accuracy of exactly 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... hyperparameter selection w/ cv here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:21:20.622629Z",
     "start_time": "2018-12-07T02:21:20.609737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression Training accuracy: 0.948357\n",
      "Baseline Logistic Regression Testing accuracy: 0.986014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegression = LogisticRegression(random_state=1234)\n",
    "logisticRegression.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Baseline Logistic Regression Training accuracy: %0.6f\" % accuracy_score(Y_train, logisticRegression.predict(X_train)))\n",
    "print(\"Baseline Logistic Regression Testing accuracy: %0.6f\" % accuracy_score(Y_test, logisticRegression.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression has learned the `most frequent` strategy that is also used in our baseline without hyperparameter modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... hyperparameter selection w/ cv here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:21:44.996708Z",
     "start_time": "2018-12-07T02:21:44.961650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Support Vector Machine Training accuracy: 1.000000\n",
      "Baseline Support Vector Machine Testing accuracy: 0.622378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "svm = SVC(random_state=1234)\n",
    "svm.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Baseline Support Vector Machine Training accuracy: %0.6f\" % accuracy_score(Y_train, svm.predict(X_train)))\n",
    "print(\"Baseline Support Vector Machine Testing accuracy: %0.6f\" % accuracy_score(Y_test, svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a support vector machine without any hyperparameter modifications also severly overfits with a training accuracy of 100%.  The test accuracy is concerning as it is exactly the same as `most frequent` baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:21:58.890420Z",
     "start_time": "2018-12-07T02:21:58.853037Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Neural Net Training accuracy: 0.537559\n",
      "Baseline Neural Net Testing accuracy: 0.531469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=1234,max_iter=1000)\n",
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Baseline Neural Net Training accuracy: %0.6f\" % accuracy_score(Y_train, mlp.predict(X_train)))\n",
    "print(\"Baseline Neural Net Testing accuracy: %0.6f\" % accuracy_score(Y_test, mlp.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preprocessing via Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for model LogisticRegression after standardizing features: 0.9859154929577465\n",
      "Test accuracy for model LogisticRegression after standardizing features: 0.993006993006993\n",
      "Train accuracy for model DecisionTreeClassifier after standardizing features: 1.0\n",
      "Test accuracy for model DecisionTreeClassifier after standardizing features: 0.958041958041958\n",
      "Train accuracy for model MLPClassifier after standardizing features: 0.9929577464788732\n",
      "Test accuracy for model MLPClassifier after standardizing features: 0.986013986013986\n",
      "Train accuracy for model SVC after standardizing features: 0.9835680751173709\n",
      "Test accuracy for model SVC after standardizing features: 0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "\n",
    "decisionTree.fit(X_train_std, Y_train)\n",
    "\n",
    "#A little sanity check to see how models perform after scaling\n",
    "models = [logisticRegression, decisionTree, mlp, svm]\n",
    "for model in models:\n",
    "    #Warm start by default is off so by calling fit it \"retrains from scratch\" which is what we want\n",
    "    model.fit(X_train_std, Y_train)\n",
    "    model_name = model.__class__.__name__\n",
    "    train_accuracy = accuracy_score(Y_train, model.predict(X_train_std))\n",
    "    test_accuracy = accuracy_score(Y_test, model.predict(X_test_std))\n",
    "    print(\"Train accuracy for model {} after standardizing features: {}\".format(model_name, train_accuracy))\n",
    "    print(\"Test accuracy for model {} after standardizing features: {}\".format(model_name, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and CV \n",
    "\n",
    "#### Perform feature selection using chi^2.\n",
    "#### Maintain a map between model type and a list of potential params e.e {\"logistic_regression\": [list of potential c values], \"neural net\": [different hidden_neural_net sizes], etc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/amogh/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "WARNING: THIS MAY TAKE A WHILE TO RUN\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "model_to_possible_params = {}\n",
    "\n",
    "#Populate model_to_params:\n",
    "#Key is model_name, value is a dictionary mapping from parameter to a list of potential values\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    tunable_params = None\n",
    "    if model_name == 'LogisticRegression':\n",
    "        tunable_params = [{'C': [0.001, 0.01,0.1,1,10,100]}]\n",
    "    elif model_name == 'DecisionTreeClassifier':\n",
    "        tunable_params = [{'max_depth':[1,2,4,8]}, {'min_samples_leaf': [1,2,3,5,8]}]\n",
    "    elif model_name == 'SVC':\n",
    "        tunable_params = [{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "    else:\n",
    "        tunable_params = [{'activation':['identity', 'logistic', 'tanh', 'relu']}, \n",
    "                          {'alpha': [1e-04, 1e-03, 1e-02, 0.05, 1, 10]}, \n",
    "                          {'hidden_layer_sizes': [(3), (3,5), (3,5,3), (3,5,5)]}]\n",
    "    \n",
    "    model_to_possible_params[model_name] = tunable_params\n",
    "    \n",
    "percentiles = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "#mapping from tuple (percentil, model) to cross validation score\n",
    "\n",
    "accuracy_results = {}\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    relevant_params = model_to_possible_params[model.__class__.__name__]\n",
    "    \n",
    "    for percentile in percentiles:\n",
    "    #ToDo: Iterate over percentile selections\n",
    "        selection = SelectPercentile(percentile=percentile, score_func=f_classif)\n",
    "        X_train_selected = selection.fit_transform(X_train_std, Y_train)\n",
    "        gs_classifier = GridSearchCV(model, relevant_params, cv=5, n_jobs=4)\n",
    "        gs_classifier.fit(X_train_selected, Y_train)\n",
    "        accuracy_results[(percentile, gs_classifier)] = gs_classifier.best_score_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9084507042253521,\n",
       " (2, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9084507042253521,\n",
       " (5, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9295774647887324,\n",
       " (10, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9460093896713615,\n",
       " (20, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9530516431924883,\n",
       " (30, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9553990610328639,\n",
       " (40, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9436619718309859,\n",
       " (50, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9413145539906104,\n",
       " (60, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9694835680751174,\n",
       " (70, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (80, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (90, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (100, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9812206572769953,\n",
       " (1, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.903755868544601,\n",
       " (2, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.903755868544601,\n",
       " (5, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9295774647887324,\n",
       " (10, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9272300469483568,\n",
       " (20, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9225352112676056,\n",
       " (30, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9272300469483568,\n",
       " (40, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.92018779342723,\n",
       " (50, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9225352112676056,\n",
       " (60, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9272300469483568,\n",
       " (70, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9272300469483568,\n",
       " (80, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9272300469483568,\n",
       " (90, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.931924882629108,\n",
       " (100, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
       "              splitter='best'),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'max_depth': [1, 2, 4, 8]}, {'min_samples_leaf': [1, 2, 3, 5, 8]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.931924882629108,\n",
       " (1, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9061032863849765,\n",
       " (2, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9061032863849765,\n",
       " (5, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9342723004694836,\n",
       " (10, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9413145539906104,\n",
       " (20, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9460093896713615,\n",
       " (30, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9577464788732394,\n",
       " (40, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9507042253521126,\n",
       " (50, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9460093896713615,\n",
       " (60, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.971830985915493,\n",
       " (70, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (80, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9788732394366197,\n",
       " (90, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9788732394366197,\n",
       " (100, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1234,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'activation': ['identity', 'logistic', 'tanh', 'relu']}, {'alpha': [0.0001, 0.001, 0.01, 0.05, 1, 10]}, {'hidden_layer_sizes': [3, (3, 5), (3, 5, 3), (3, 5, 5)]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9741784037558685,\n",
       " (1, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9084507042253521,\n",
       " (2, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9084507042253521,\n",
       " (5, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9413145539906104,\n",
       " (10, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9436619718309859,\n",
       " (20, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9507042253521126,\n",
       " (30, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9530516431924883,\n",
       " (40, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9460093896713615,\n",
       " (50, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9530516431924883,\n",
       " (60, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9694835680751174,\n",
       " (70, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (80, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9765258215962441,\n",
       " (90, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9741784037558685,\n",
       " (100, GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=1234, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "         fit_params=None, iid=True, n_jobs=4,\n",
       "         param_grid=[{'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}, {'C': [0.001, 0.01, 0.1, 1, 10, 100]}],\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=0)): 0.9694835680751174}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's kind of interesting to note, in general more features results in a better accuracy. The best models usually used 100 percentile of features (all features). This implies that there is little correlation between the features (little \"redundancy\" in feature set). We can do a quick check of the correlation matrix to see if any features are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "radius_mean                 1.000000      0.323782        0.997855   0.987357   \n",
      "texture_mean                0.323782      1.000000        0.329533   0.321086   \n",
      "perimeter_mean              0.997855      0.329533        1.000000   0.986507   \n",
      "area_mean                   0.987357      0.321086        0.986507   1.000000   \n",
      "smoothness_mean             0.170581     -0.023389        0.207278   0.177028   \n",
      "compactness_mean            0.506124      0.236702        0.556936   0.498502   \n",
      "concavity_mean              0.676764      0.302418        0.716136   0.685983   \n",
      "concave points_mean         0.822529      0.293464        0.850977   0.823269   \n",
      "symmetry_mean               0.147741      0.071401        0.183027   0.151293   \n",
      "fractal_dimension_mean     -0.311631     -0.076437       -0.261477  -0.283110   \n",
      "radius_se                   0.679090      0.275869        0.691765   0.732562   \n",
      "texture_se                 -0.097317      0.386358       -0.086761  -0.066280   \n",
      "perimeter_se                0.674172      0.281673        0.693135   0.726628   \n",
      "area_se                     0.735864      0.259845        0.744983   0.800086   \n",
      "smoothness_se              -0.222600      0.006614       -0.202694  -0.166777   \n",
      "compactness_se              0.206000      0.191975        0.250744   0.212583   \n",
      "concavity_se                0.194204      0.143293        0.228082   0.207660   \n",
      "concave points_se           0.376169      0.163851        0.407217   0.372320   \n",
      "symmetry_se                -0.104321      0.009127       -0.081629  -0.072497   \n",
      "fractal_dimension_se       -0.042641      0.054458       -0.005523  -0.019887   \n",
      "radius_worst                0.969539      0.352573        0.969476   0.962746   \n",
      "texture_worst               0.297008      0.912045        0.303038   0.287489   \n",
      "perimeter_worst             0.965137      0.358040        0.970387   0.959120   \n",
      "area_worst                  0.941082      0.343546        0.941550   0.959213   \n",
      "smoothness_worst            0.119616      0.077503        0.150549   0.123523   \n",
      "compactness_worst           0.413463      0.277830        0.455774   0.390410   \n",
      "concavity_worst             0.526911      0.301025        0.563879   0.512606   \n",
      "concave points_worst        0.744214      0.295316        0.771241   0.722017   \n",
      "symmetry_worst              0.163953      0.105008        0.189115   0.143570   \n",
      "fractal_dimension_worst     0.007066      0.119205        0.051019   0.003738   \n",
      "\n",
      "                         smoothness_mean  compactness_mean  concavity_mean  \\\n",
      "radius_mean                     0.170581          0.506124        0.676764   \n",
      "texture_mean                   -0.023389          0.236702        0.302418   \n",
      "perimeter_mean                  0.207278          0.556936        0.716136   \n",
      "area_mean                       0.177028          0.498502        0.685983   \n",
      "smoothness_mean                 1.000000          0.659123        0.521984   \n",
      "compactness_mean                0.659123          1.000000        0.883121   \n",
      "concavity_mean                  0.521984          0.883121        1.000000   \n",
      "concave points_mean             0.553695          0.831135        0.921391   \n",
      "symmetry_mean                   0.557775          0.602641        0.500667   \n",
      "fractal_dimension_mean          0.584792          0.565369        0.336783   \n",
      "radius_se                       0.301467          0.497473        0.631925   \n",
      "texture_se                      0.068406          0.046205        0.076218   \n",
      "perimeter_se                    0.296092          0.548905        0.660391   \n",
      "area_se                         0.246552          0.455653        0.617427   \n",
      "smoothness_se                   0.332375          0.135299        0.098564   \n",
      "compactness_se                  0.318943          0.738722        0.670279   \n",
      "concavity_se                    0.248396          0.570517        0.691270   \n",
      "concave points_se               0.380676          0.642262        0.683260   \n",
      "symmetry_se                     0.200774          0.229977        0.178009   \n",
      "fractal_dimension_se            0.283607          0.507318        0.449301   \n",
      "radius_worst                    0.213120          0.535315        0.688236   \n",
      "texture_worst                   0.036072          0.248133        0.299879   \n",
      "perimeter_worst                 0.238853          0.590210        0.729565   \n",
      "area_worst                      0.206718          0.509604        0.675987   \n",
      "smoothness_worst                0.805324          0.565541        0.448822   \n",
      "compactness_worst               0.472468          0.865809        0.754968   \n",
      "concavity_worst                 0.434926          0.816275        0.884103   \n",
      "concave points_worst            0.503053          0.815573        0.861323   \n",
      "symmetry_worst                  0.394309          0.510223        0.409464   \n",
      "fractal_dimension_worst         0.499316          0.687382        0.514930   \n",
      "\n",
      "                         concave points_mean  symmetry_mean  \\\n",
      "radius_mean                         0.822529       0.147741   \n",
      "texture_mean                        0.293464       0.071401   \n",
      "perimeter_mean                      0.850977       0.183027   \n",
      "area_mean                           0.823269       0.151293   \n",
      "smoothness_mean                     0.553695       0.557775   \n",
      "compactness_mean                    0.831135       0.602641   \n",
      "concavity_mean                      0.921391       0.500667   \n",
      "concave points_mean                 1.000000       0.462497   \n",
      "symmetry_mean                       0.462497       1.000000   \n",
      "fractal_dimension_mean              0.166917       0.479921   \n",
      "radius_se                           0.698050       0.303379   \n",
      "texture_se                          0.021480       0.128053   \n",
      "perimeter_se                        0.710650       0.313893   \n",
      "area_se                             0.690299       0.223970   \n",
      "smoothness_se                       0.027653       0.187321   \n",
      "compactness_se                      0.490424       0.421659   \n",
      "concavity_se                        0.439167       0.342627   \n",
      "concave points_se                   0.615634       0.393298   \n",
      "symmetry_se                         0.095351       0.449137   \n",
      "fractal_dimension_se                0.257584       0.331786   \n",
      "radius_worst                        0.830318       0.185728   \n",
      "texture_worst                       0.292752       0.090651   \n",
      "perimeter_worst                     0.855923       0.219169   \n",
      "area_worst                          0.809630       0.177193   \n",
      "smoothness_worst                    0.452753       0.426675   \n",
      "compactness_worst                   0.667454       0.473200   \n",
      "concavity_worst                     0.752399       0.433721   \n",
      "concave points_worst                0.910155       0.430297   \n",
      "symmetry_worst                      0.375744       0.699826   \n",
      "fractal_dimension_worst             0.368661       0.438413   \n",
      "\n",
      "                         fractal_dimension_mean           ...             \\\n",
      "radius_mean                           -0.311631           ...              \n",
      "texture_mean                          -0.076437           ...              \n",
      "perimeter_mean                        -0.261477           ...              \n",
      "area_mean                             -0.283110           ...              \n",
      "smoothness_mean                        0.584792           ...              \n",
      "compactness_mean                       0.565369           ...              \n",
      "concavity_mean                         0.336783           ...              \n",
      "concave points_mean                    0.166917           ...              \n",
      "symmetry_mean                          0.479921           ...              \n",
      "fractal_dimension_mean                 1.000000           ...              \n",
      "radius_se                              0.000111           ...              \n",
      "texture_se                             0.164174           ...              \n",
      "perimeter_se                           0.039830           ...              \n",
      "area_se                               -0.090170           ...              \n",
      "smoothness_se                          0.401964           ...              \n",
      "compactness_se                         0.559837           ...              \n",
      "concavity_se                           0.446630           ...              \n",
      "concave points_se                      0.341198           ...              \n",
      "symmetry_se                            0.345007           ...              \n",
      "fractal_dimension_se                   0.688132           ...              \n",
      "radius_worst                          -0.253691           ...              \n",
      "texture_worst                         -0.051269           ...              \n",
      "perimeter_worst                       -0.205151           ...              \n",
      "area_worst                            -0.231854           ...              \n",
      "smoothness_worst                       0.504942           ...              \n",
      "compactness_worst                      0.458798           ...              \n",
      "concavity_worst                        0.346234           ...              \n",
      "concave points_worst                   0.175325           ...              \n",
      "symmetry_worst                         0.334019           ...              \n",
      "fractal_dimension_worst                0.767297           ...              \n",
      "\n",
      "                         radius_worst  texture_worst  perimeter_worst  \\\n",
      "radius_mean                  0.969539       0.297008         0.965137   \n",
      "texture_mean                 0.352573       0.912045         0.358040   \n",
      "perimeter_mean               0.969476       0.303038         0.970387   \n",
      "area_mean                    0.962746       0.287489         0.959120   \n",
      "smoothness_mean              0.213120       0.036072         0.238853   \n",
      "compactness_mean             0.535315       0.248133         0.590210   \n",
      "concavity_mean               0.688236       0.299879         0.729565   \n",
      "concave points_mean          0.830318       0.292752         0.855923   \n",
      "symmetry_mean                0.185728       0.090651         0.219169   \n",
      "fractal_dimension_mean      -0.253691      -0.051269        -0.205151   \n",
      "radius_se                    0.715065       0.194799         0.719684   \n",
      "texture_se                  -0.111690       0.409003        -0.102242   \n",
      "perimeter_se                 0.697201       0.200371         0.721031   \n",
      "area_se                      0.757373       0.196497         0.761213   \n",
      "smoothness_se               -0.230691      -0.074743        -0.217304   \n",
      "compactness_se               0.204607       0.143003         0.260516   \n",
      "concavity_se                 0.186904       0.100241         0.226680   \n",
      "concave points_se            0.358127       0.086741         0.394999   \n",
      "symmetry_se                 -0.128121      -0.077473        -0.103753   \n",
      "fractal_dimension_se        -0.037488      -0.003195        -0.001000   \n",
      "radius_worst                 1.000000       0.359921         0.993708   \n",
      "texture_worst                0.359921       1.000000         0.365098   \n",
      "perimeter_worst              0.993708       0.365098         1.000000   \n",
      "area_worst                   0.984015       0.345842         0.977578   \n",
      "smoothness_worst             0.216574       0.225429         0.236775   \n",
      "compactness_worst            0.475820       0.360832         0.529408   \n",
      "concavity_worst              0.573975       0.368366         0.618344   \n",
      "concave points_worst         0.787424       0.359755         0.816322   \n",
      "symmetry_worst               0.243529       0.233027         0.269493   \n",
      "fractal_dimension_worst      0.093492       0.219122         0.138957   \n",
      "\n",
      "                         area_worst  smoothness_worst  compactness_worst  \\\n",
      "radius_mean                0.941082          0.119616           0.413463   \n",
      "texture_mean               0.343546          0.077503           0.277830   \n",
      "perimeter_mean             0.941550          0.150549           0.455774   \n",
      "area_mean                  0.959213          0.123523           0.390410   \n",
      "smoothness_mean            0.206718          0.805324           0.472468   \n",
      "compactness_mean           0.509604          0.565541           0.865809   \n",
      "concavity_mean             0.675987          0.448822           0.754968   \n",
      "concave points_mean        0.809630          0.452753           0.667454   \n",
      "symmetry_mean              0.177193          0.426675           0.473200   \n",
      "fractal_dimension_mean    -0.231854          0.504942           0.458798   \n",
      "radius_se                  0.751548          0.141919           0.287103   \n",
      "texture_se                -0.083195         -0.073658          -0.092439   \n",
      "perimeter_se               0.730713          0.130054           0.341919   \n",
      "area_se                    0.811408          0.125389           0.283257   \n",
      "smoothness_se             -0.182195          0.314457          -0.055558   \n",
      "compactness_se             0.199371          0.227394           0.678780   \n",
      "concavity_se               0.188353          0.168481           0.484858   \n",
      "concave points_se          0.342271          0.215351           0.452888   \n",
      "symmetry_se               -0.110343         -0.012662           0.060255   \n",
      "fractal_dimension_se      -0.022736          0.170568           0.390159   \n",
      "radius_worst               0.984015          0.216574           0.475820   \n",
      "texture_worst              0.345842          0.225429           0.360832   \n",
      "perimeter_worst            0.977578          0.236775           0.529408   \n",
      "area_worst                 1.000000          0.209145           0.438296   \n",
      "smoothness_worst           0.209145          1.000000           0.568187   \n",
      "compactness_worst          0.438296          0.568187           1.000000   \n",
      "concavity_worst            0.543331          0.518523           0.892261   \n",
      "concave points_worst       0.747419          0.547691           0.801080   \n",
      "symmetry_worst             0.209146          0.493838           0.614441   \n",
      "fractal_dimension_worst    0.079647          0.617624           0.810455   \n",
      "\n",
      "                         concavity_worst  concave points_worst  \\\n",
      "radius_mean                     0.526911              0.744214   \n",
      "texture_mean                    0.301025              0.295316   \n",
      "perimeter_mean                  0.563879              0.771241   \n",
      "area_mean                       0.512606              0.722017   \n",
      "smoothness_mean                 0.434926              0.503053   \n",
      "compactness_mean                0.816275              0.815573   \n",
      "concavity_mean                  0.884103              0.861323   \n",
      "concave points_mean             0.752399              0.910155   \n",
      "symmetry_mean                   0.433721              0.430297   \n",
      "fractal_dimension_mean          0.346234              0.175325   \n",
      "radius_se                       0.380585              0.531062   \n",
      "texture_se                     -0.068956             -0.119638   \n",
      "perimeter_se                    0.418899              0.554897   \n",
      "area_se                         0.385100              0.538166   \n",
      "smoothness_se                  -0.058298             -0.102007   \n",
      "compactness_se                  0.639147              0.483208   \n",
      "concavity_se                    0.662564              0.440472   \n",
      "concave points_se               0.549592              0.602450   \n",
      "symmetry_se                     0.037119             -0.030413   \n",
      "fractal_dimension_se            0.379975              0.215204   \n",
      "radius_worst                    0.573975              0.787424   \n",
      "texture_worst                   0.368366              0.359755   \n",
      "perimeter_worst                 0.618344              0.816322   \n",
      "area_worst                      0.543331              0.747419   \n",
      "smoothness_worst                0.518523              0.547691   \n",
      "compactness_worst               0.892261              0.801080   \n",
      "concavity_worst                 1.000000              0.855434   \n",
      "concave points_worst            0.855434              1.000000   \n",
      "symmetry_worst                  0.532520              0.502528   \n",
      "fractal_dimension_worst         0.686511              0.511114   \n",
      "\n",
      "                         symmetry_worst  fractal_dimension_worst  \n",
      "radius_mean                    0.163953                 0.007066  \n",
      "texture_mean                   0.105008                 0.119205  \n",
      "perimeter_mean                 0.189115                 0.051019  \n",
      "area_mean                      0.143570                 0.003738  \n",
      "smoothness_mean                0.394309                 0.499316  \n",
      "compactness_mean               0.510223                 0.687382  \n",
      "concavity_mean                 0.409464                 0.514930  \n",
      "concave points_mean            0.375744                 0.368661  \n",
      "symmetry_mean                  0.699826                 0.438413  \n",
      "fractal_dimension_mean         0.334019                 0.767297  \n",
      "radius_se                      0.094543                 0.049559  \n",
      "texture_se                    -0.128215                -0.045655  \n",
      "perimeter_se                   0.109930                 0.085433  \n",
      "area_se                        0.074126                 0.017539  \n",
      "smoothness_se                 -0.107342                 0.101480  \n",
      "compactness_se                 0.277878                 0.590973  \n",
      "concavity_se                   0.197788                 0.439329  \n",
      "concave points_se              0.143116                 0.310655  \n",
      "symmetry_se                    0.389402                 0.078079  \n",
      "fractal_dimension_se           0.111094                 0.591328  \n",
      "radius_worst                   0.243529                 0.093492  \n",
      "texture_worst                  0.233027                 0.219122  \n",
      "perimeter_worst                0.269493                 0.138957  \n",
      "area_worst                     0.209146                 0.079647  \n",
      "smoothness_worst               0.493838                 0.617624  \n",
      "compactness_worst              0.614441                 0.810455  \n",
      "concavity_worst                0.532520                 0.686511  \n",
      "concave points_worst           0.502528                 0.511114  \n",
      "symmetry_worst                 1.000000                 0.537848  \n",
      "fractal_dimension_worst        0.537848                 1.000000  \n",
      "\n",
      "[30 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = data_cleaned.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model according to cross validation results was LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) with accuracy 0.9812206572769953, percentile features 100\n",
      "The worst model according to cross validation results was DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=1234,\n",
      "            splitter='best') with accuracy 0.903755868544601, percentile features 1\n"
     ]
    }
   ],
   "source": [
    "sorted_by_accuracy = sorted(accuracy_results.items(), key=lambda kv: kv[1])\n",
    "#best worst models are tuple ((percentile, model), accuracy)\n",
    "best_model_tup = sorted_by_accuracy[-1]\n",
    "worst_model_tup = sorted_by_accuracy[0]\n",
    "print(\"The best model according to cross validation results was {} with accuracy {}, percentile features {}\".format(best_model_tup[0][1].estimator, best_model_tup[1], best_model_tup[0][0]))\n",
    "print(\"The worst model according to cross validation results was {} with accuracy {}, percentile features {}\".format(worst_model_tup[0][1].estimator, worst_model_tup[1], worst_model_tup[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993006993006993"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = best_model_tup[0][1].estimator\n",
    "best_model.fit(X_train_std, Y_train)\n",
    "accuracy_score(Y_test, best_model.predict(X_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
